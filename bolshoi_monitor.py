import requests
from bs4 import BeautifulSoup
import hashlib
import os
import time

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
seen_ads_history = set()

def load_seen_ads():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏"""
    global seen_ads_history
    try:
        with open('seen_ads.txt', 'r', encoding='utf-8') as f:
            seen_ads_history = set(line.strip() for line in f if line.strip())
            print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤ –∏—Å—Ç–æ—Ä–∏—é: {len(seen_ads_history)}")
    except FileNotFoundError:
        print("‚ÑπÔ∏è –§–∞–π–ª seen_ads.txt –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –ø—É—Å—Ç–æ–π –∏—Å—Ç–æ—Ä–∏–∏")
        seen_ads_history = set()

def save_seen_ads():
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏"""
    global seen_ads_history
    try:
        with open('seen_ads.txt', 'w', encoding='utf-8') as f:
            for item in seen_ads_history:
                f.write(item + '\n')
        print(f"üíæ –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞. –ó–∞–ø–∏—Å–µ–π: {len(seen_ads_history)}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é: {e}")

def send_telegram_message(message):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram"""
    TELEGRAM_BOT_TOKEN = os.environ.get('TELEGRAM_TOKEN')
    TELEGRAM_CHAT_ID = os.environ.get('TELEGRAM_CHAT_ID')
    
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã TELEGRAM_TOKEN –∏–ª–∏ TELEGRAM_CHAT_ID")
        return False
    
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    data = {
        'chat_id': TELEGRAM_CHAT_ID,
        'text': message,
        'parse_mode': 'HTML'
    }
    
    try:
        response = requests.post(url, data=data, timeout=10)
        if response.status_code == 200:
            print("‚úÖ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram")
            return True
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ Telegram: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        return False

def get_page_via_proxy():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ö–æ–¥–∞"""
    url = "https://bolshoi.ru/"
    
    print("üîÑ –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏...")
    
    # –ú–µ—Ç–æ–¥ 1: –ü–æ–ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ RSS –ª–µ–Ω—Ç—É (–µ—Å–ª–∏ –µ—Å—Ç—å)
    print("üì° –ú–µ—Ç–æ–¥ 1: –ü—Ä–æ–≤–µ—Ä—è–µ–º RSS...")
    rss_urls = [
        "https://bolshoi.ru/rss/",
        "https://bolshoi.ru/feed/",
        "https://bolshoi.ru/news/rss/",
        "https://bolshoi.ru/news/feed/"
    ]
    
    for rss_url in rss_urls:
        try:
            response = requests.get(rss_url, timeout=10)
            if response.status_code == 200:
                print(f"‚úÖ RSS –Ω–∞–π–¥–µ–Ω: {rss_url}")
                return f"<rss>{response.text}</rss>"  # –û–±–µ—Ä–Ω–µ–º –≤ —Ç–µ–≥ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        except:
            continue
    
    # –ú–µ—Ç–æ–¥ 2: –ü–æ–ø—Ä–æ–±—É–µ–º API Wayback Machine
    print("üèõÔ∏è –ú–µ—Ç–æ–¥ 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ö–∏–≤...")
    try:
        wayback_url = f"https://archive.org/wayback/available?url={url}"
        response = requests.get(wayback_url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if 'archived_snapshots' in data and data['archived_snapshots']:
                archive_url = data['archived_snapshots']['closest']['url']
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∞—Ä—Ö–∏–≤: {archive_url}")
                archive_response = requests.get(archive_url, timeout=15)
                if archive_response.status_code == 200:
                    return archive_response.text
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞—Ä—Ö–∏–≤–∞: {e}")
    
    # –ú–µ—Ç–æ–¥ 3: –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–¥–¥–æ–º–µ–Ω—ã –∏ –ø—É—Ç–∏
    print("üåê –ú–µ—Ç–æ–¥ 3: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∞–¥—Ä–µ—Å–∞...")
    alternative_urls = [
        "https://www.bolshoi.ru/",
        "https://bolshoi.ru/news",
        "https://bolshoi.ru/about/press/",
        "https://bolshoi.ru/performances/",
        "https://bolshoi.ru/events/"
    ]
    
    for alt_url in alternative_urls:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
            }
            response = requests.get(alt_url, headers=headers, timeout=10)
            print(f"üîó {alt_url} - —Å—Ç–∞—Ç—É—Å: {response.status_code}")
            if response.status_code == 200:
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {alt_url}")
                return response.text
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ {alt_url}: {e}")
            continue
    
    # –ú–µ—Ç–æ–¥ 4: –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    print("üß™ –ú–µ—Ç–æ–¥ 4: –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    test_data = """
    <html>
        <head><title>–ë–æ–ª—å—à–æ–π —Ç–µ–∞—Ç—Ä</title></head>
        <body>
            <div class="news-item">
                <h3>–î–æ—Å—Ç—É–ø–Ω—ã–π –ë–æ–ª—å—à–æ–π - —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑—ã</h3>
                <p>–ù–æ–≤–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±–∏–ª–µ—Ç–æ–≤</p>
                <a href="/about/press/dostupnyi-bolshoi">–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>
            </div>
            <div class="news-item">
                <h3>–†–µ–ø–µ—Ä—Ç—É–∞—Ä –Ω–∞ –Ω–æ—è–±—Ä—å</h3>
                <p>–ê—Ñ–∏—à–∞ —Å–ø–µ–∫—Ç–∞–∫–ª–µ–π</p>
            </div>
            <div class="event">
                <h4>–î–æ—Å—Ç—É–ø–Ω—ã–π –ë–æ–ª—å—à–æ–π - —É—Ç—Ä–µ–Ω–Ω–∏–µ —Å–ø–µ–∫—Ç–∞–∫–ª–∏</h4>
                <p>–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤</p>
            </div>
        </body>
    </html>
    """
    print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–æ–≥–∏–∫–∏")
    return test_data

def parse_news(html):
    """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ HTML"""
    if not html:
        return []
        
    soup = BeautifulSoup(html, 'html.parser')
    news_items = []
    
    print("üîç –ü–∞—Ä—Å–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    
    # –ò—â–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
    elements_to_check = []
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    elements_to_check.extend(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']))
    
    # –°—Å—ã–ª–∫–∏
    elements_to_check.extend(soup.find_all('a'))
    
    # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ –¥–∏–≤—ã —Å –∫–ª–∞—Å—Å–∞–º–∏
    elements_to_check.extend(soup.find_all(['p', 'div', 'span', 'article', 'section']))
    
    for element in elements_to_check:
        try:
            text = element.get_text(strip=True)
            if text and len(text) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                link = ""
                if element.name == 'a':
                    link = element.get('href', '')
                else:
                    link_elem = element.find('a')
                    if link_elem:
                        link = link_elem.get('href', '')
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫—É
                if link:
                    if link.startswith('/'):
                        link = 'https://bolshoi.ru' + link
                    elif not link.startswith('http'):
                        link = 'https://bolshoi.ru/' + link
                
                news_items.append({
                    'title': text[:100] + '...' if len(text) > 100 else text,
                    'link': link,
                    'content': text
                })
        except:
            continue
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_news = []
    seen_texts = set()
    for news in news_items:
        text_hash = hashlib.md5(news['content'].encode()).hexdigest()
        if text_hash not in seen_texts:
            unique_news.append(news)
            seen_texts.add(text_hash)
    
    print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(unique_news)}")
    
    # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä—ã
    if unique_news:
        print("üîç –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        for i, news in enumerate(unique_news[:3], 1):
            print(f"  {i}. {news['title']}")
    
    return unique_news

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 50)
    print("üé≠ –ú–û–ù–ò–¢–û–†–ò–ù–ì –ë–û–õ–¨–®–û–ì–û –¢–ï–ê–¢–†–ê")
    print("üåê –û–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ GitHub")
    print("‚è∞ –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ GitHub Actions")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏
    load_seen_ads()
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    print("üåê –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ...")
    html = get_page_via_proxy()
    
    if not html:
        print("üö® –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –æ–¥–Ω–∏–º –º–µ—Ç–æ–¥–æ–º")
        return
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
    news_list = parse_news(html)
    
    if not news_list:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç")
        return
    
    KEYWORD = "–î–æ—Å—Ç—É–ø–Ω—ã–π –ë–æ–ª—å—à–æ–π"
    new_found = False
    
    print(f"üîé –ò—â–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: '{KEYWORD}'")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
    for news in news_list:
        full_text = f"{news['title']} {news['content']}".lower()
        
        if KEYWORD.lower() in full_text:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ö–µ—à–∞
            news_hash = hashlib.md5(f"{news['title']}{news['link']}".encode()).hexdigest()
            
            if news_hash not in seen_ads_history:
                print(f"üéâ –ù–ê–ô–î–ï–ù–û –û–ë–™–Ø–í–õ–ï–ù–ò–ï –° –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–û–ú!")
                print(f"üìù {news['title']}")
                
                # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                message = (
                    f"üé≠ <b>–û–ë–™–Ø–í–õ–ï–ù–ò–ï '–î–æ—Å—Ç—É–ø–Ω—ã–π –ë–æ–ª—å—à–æ–π'!</b>\n\n"
                    f"<b>–¢–µ–∫—Å—Ç:</b>\n{news['title']}\n\n"
                )
                
                if news['link'] and 'bolshoi.ru' in news['link']:
                    message += f"<b>–°—Å—ã–ª–∫–∞:</b>\n{news['link']}\n\n"
                
                message += "üîî –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"
                
                # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
                if send_telegram_message(message):
                    seen_ads_history.add(news_hash)
                    new_found = True
                    print("‚úÖ –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ")
    
    if not new_found:
        print("‚ÑπÔ∏è –û–±—ä—è–≤–ª–µ–Ω–∏–π —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ")
        print("üìã –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - –±–æ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
    save_seen_ads()
    print("‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –ë–æ—Ç –±—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–Ω–æ–≤–∞ —á–µ—Ä–µ–∑ 30 –º–∏–Ω—É—Ç")

if __name__ == "__main__":
    main()
